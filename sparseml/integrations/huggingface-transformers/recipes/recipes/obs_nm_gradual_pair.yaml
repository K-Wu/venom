modifiers:
  - !EpochRangeModifier
    start_epoch: 0
    end_epoch: 50

training_modifiers:
  - !LearningRateFunctionModifier
    start_epoch: 0.0
    end_epoch: 2.0
    lr_func: linear
    init_lr: 8e-5
    final_lr: 8e-6
  - !LearningRateFunctionModifier
    start_epoch: 2.0
    end_epoch: 50.0
    lr_func: cyclic_linear
    cycle_epochs: 4.0
    init_lr: 8e-5
    final_lr: 8e-6

  - !OBSnmpairPruningModifier
    damp: 1e-07
    end_epoch: 30.0
    final_sparsity: 0.875
    fisher_block_size: 32
    global_sparsity: True
    init_sparsity: 0.4375
    inter_func: linear
    leave_enabled: True
    mask_type: unstructured
    num_grads: 1024
    num_recomputations: 1
    params: ['re:bert.encoder.layer.*.attention.self.query.weight', 're:bert.encoder.layer.*.attention.self.key.weight', 're:bert.encoder.layer.*.attention.self.value.weight', 're:bert.encoder.layer.*.attention.output.dense.weight', 're:bert.encoder.layer.*.intermediate.dense.weight', 're:bert.encoder.layer.*.output.dense.weight']
    start_epoch: 2.0
    update_frequency: 4.0


 #- !OBSnmpairPruningModifier
 #    damp: 1e-07
 #    end_epoch: 14.0
 #    final_sparsity: 0.75
 #    fisher_block_size: 32
 #    global_sparsity: True
 #    init_sparsity: 0.375
 #    inter_func: linear
 #    leave_enabled: True
 #    mask_type: unstructured
 #    num_grads: 1024
 #    num_recomputations: 1
 #    params: ['re:bert.encoder.layer.*.attention.self.query.weight', 're:bert.encoder.layer.*.attention.self.key.weight', 're:bert.encoder.layer.*.attention.self.value.weight', 're:bert.encoder.layer.*.attention.output.dense.weight', 're:bert.encoder.layer.*.intermediate.dense.weight', 're:bert.encoder.layer.*.output.dense.weight']
 #    start_epoch: 2.0
 #    update_frequency: 4.0

distillation_modifiers:
  - !DistillationModifier
     hardness: 1.0
     temperature: 2.0
     distill_output_keys: [start_logits, end_logits]
